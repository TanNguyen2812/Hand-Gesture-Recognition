C:\Users\Admin\AppData\Local\Programs\Python\Python39\python.exe "C:/Users/Admin/Desktop/ComputerVision/BTL/New folder/LSTM_model.py"
Epoch: 0 train loss: 2.2971 val loss: 2.2925
C:\Users\Admin\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\optim\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Epoch: 1 train loss: 2.2857 val loss: 2.2912
Epoch: 2 train loss: 2.2799 val loss: 2.2928
Epoch: 3 train loss: 2.2775 val loss: 2.2932
Epoch: 4 train loss: 2.2782 val loss: 2.2969
Epoch: 5 train loss: 2.2771 val loss: 2.2939
Epoch: 6 train loss: 2.2764 val loss: 2.2952
Epoch: 7 train loss: 2.2771 val loss: 2.2950
Epoch: 8 train loss: 2.2776 val loss: 2.2961
Epoch: 9 train loss: 2.2763 val loss: 2.2966
Epoch: 10 train loss: 2.2764 val loss: 2.3023
Epoch: 11 train loss: 2.2758 val loss: 2.2935
Epoch: 12 train loss: 2.2768 val loss: 2.2948
Epoch: 13 train loss: 2.2764 val loss: 2.2999
Epoch: 14 train loss: 2.2758 val loss: 2.2965
Epoch: 15 train loss: 2.2756 val loss: 2.2919
Epoch: 16 train loss: 2.2765 val loss: 2.2957
Epoch: 17 train loss: 2.2754 val loss: 2.2978
Epoch: 18 train loss: 2.2740 val loss: 2.2942
Epoch: 19 train loss: 2.2748 val loss: 2.2916
Epoch: 20 train loss: 2.2750 val loss: 2.2909
Epoch: 21 train loss: 2.2727 val loss: 2.2928
Epoch: 22 train loss: 2.2714 val loss: 2.2923
Epoch: 23 train loss: 2.2730 val loss: 2.2912
Epoch: 24 train loss: 2.2701 val loss: 2.2894
Epoch: 25 train loss: 2.2691 val loss: 2.2903
Epoch: 26 train loss: 2.2674 val loss: 2.2864
Epoch: 27 train loss: 2.2638 val loss: 2.2815
Epoch: 28 train loss: 2.2599 val loss: 2.2792
Epoch: 29 train loss: 2.2520 val loss: 2.2676
Epoch: 30 train loss: 2.2345 val loss: 2.2452
Epoch: 31 train loss: 2.1914 val loss: 2.1865
Epoch: 32 train loss: 2.0955 val loss: 2.0822
Epoch: 33 train loss: 1.9538 val loss: 1.9774
Epoch: 34 train loss: 1.8293 val loss: 1.8964
Epoch: 35 train loss: 1.7947 val loss: 1.8900
Epoch: 36 train loss: 1.7949 val loss: 1.8100
Epoch: 37 train loss: 1.7813 val loss: 1.7810
Epoch: 38 train loss: 1.7088 val loss: 1.7421
Epoch: 39 train loss: 1.6728 val loss: 1.6889
Epoch: 40 train loss: 1.6042 val loss: 1.6289
Epoch: 41 train loss: 1.5137 val loss: 1.5324
Epoch: 42 train loss: 1.4299 val loss: 1.4463
Epoch: 43 train loss: 1.3793 val loss: 1.4036
Epoch: 44 train loss: 1.2931 val loss: 1.3163
Epoch: 45 train loss: 1.2640 val loss: 1.2240
Epoch: 46 train loss: 1.1317 val loss: 1.2079
Epoch: 47 train loss: 1.1671 val loss: 1.1226
Epoch: 48 train loss: 1.0828 val loss: 1.1101
Epoch: 49 train loss: 0.9844 val loss: 1.0917
Epoch: 50 train loss: 1.0047 val loss: 0.9328
Epoch: 51 train loss: 0.9888 val loss: 0.9283
Epoch: 52 train loss: 0.9305 val loss: 0.8304
Epoch: 53 train loss: 0.8760 val loss: 0.8067
Epoch: 54 train loss: 0.8570 val loss: 0.8780
Epoch: 55 train loss: 0.7807 val loss: 0.8650
Epoch: 56 train loss: 0.7548 val loss: 0.7178
Epoch: 57 train loss: 0.7193 val loss: 0.6559
Epoch: 58 train loss: 0.6461 val loss: 0.6405
Epoch: 59 train loss: 0.6614 val loss: 0.7063
Epoch: 60 train loss: 0.6105 val loss: 0.5936
Epoch: 61 train loss: 0.5079 val loss: 0.4968
Epoch: 62 train loss: 0.4721 val loss: 0.4424
Epoch: 63 train loss: 0.4380 val loss: 0.3995
Epoch: 64 train loss: 0.4071 val loss: 0.4504
Epoch: 65 train loss: 0.4068 val loss: 0.3288
Epoch: 66 train loss: 0.4059 val loss: 0.3223
Epoch: 67 train loss: 0.3438 val loss: 0.4073
Epoch: 68 train loss: 0.3595 val loss: 0.2894
Epoch: 69 train loss: 0.4407 val loss: 0.2731
Epoch: 70 train loss: 0.3317 val loss: 0.3479
Epoch: 71 train loss: 0.3267 val loss: 0.2155
Epoch: 72 train loss: 0.3825 val loss: 0.2125
Epoch: 73 train loss: 0.2978 val loss: 0.1689
Epoch: 74 train loss: 0.2635 val loss: 0.2397
Epoch: 75 train loss: 0.2797 val loss: 0.2100
Epoch: 76 train loss: 0.3048 val loss: 0.2505
Epoch: 77 train loss: 0.2908 val loss: 0.2256
Epoch: 78 train loss: 0.2282 val loss: 0.1629
Epoch: 79 train loss: 0.2642 val loss: 0.2043
Epoch: 80 train loss: 0.2235 val loss: 0.1375
Epoch: 81 train loss: 0.1576 val loss: 0.1195
Epoch: 82 train loss: 0.1625 val loss: 0.1720
Epoch: 83 train loss: 0.1912 val loss: 0.1026
Epoch: 84 train loss: 0.1869 val loss: 0.1260
Epoch: 85 train loss: 0.1262 val loss: 0.1161
Epoch: 86 train loss: 0.1207 val loss: 0.1138
Epoch: 87 train loss: 0.1314 val loss: 0.1072
Epoch: 88 train loss: 0.1473 val loss: 0.0597
Epoch: 89 train loss: 0.1326 val loss: 0.1244
Epoch: 90 train loss: 0.1147 val loss: 0.1195
Epoch: 91 train loss: 0.2095 val loss: 0.1400
Epoch: 92 train loss: 0.1362 val loss: 0.0874
Epoch: 93 train loss: 0.1358 val loss: 0.0954
Epoch: 94 train loss: 0.1077 val loss: 0.1131
Epoch: 95 train loss: 0.1292 val loss: 0.0583
Epoch: 96 train loss: 0.1098 val loss: 0.0866
Epoch: 97 train loss: 0.2290 val loss: 0.0610
Epoch: 98 train loss: 0.1696 val loss: 0.1502
Epoch: 99 train loss: 0.1155 val loss: 0.1019
Epoch: 100 train loss: 0.0948 val loss: 0.0582
Epoch: 101 train loss: 0.0613 val loss: 0.0705
Epoch: 102 train loss: 0.0688 val loss: 0.0922
Epoch: 103 train loss: 0.0699 val loss: 0.0354
Epoch: 104 train loss: 0.0828 val loss: 0.0535
Epoch: 105 train loss: 0.0744 val loss: 0.0928
Epoch: 106 train loss: 0.0499 val loss: 0.0503
Epoch: 107 train loss: 0.0677 val loss: 0.0931
Epoch: 108 train loss: 0.0980 val loss: 0.1340
Epoch: 109 train loss: 0.0783 val loss: 0.0505
Epoch: 110 train loss: 0.0744 val loss: 0.0802
Epoch: 111 train loss: 0.0378 val loss: 0.0414
Epoch: 112 train loss: 0.0622 val loss: 0.1328
Epoch: 113 train loss: 0.0568 val loss: 0.1022
Epoch: 114 train loss: 0.0267 val loss: 0.0374
Epoch: 115 train loss: 0.0571 val loss: 0.0440
Epoch: 116 train loss: 0.0398 val loss: 0.0958
Epoch: 117 train loss: 0.0379 val loss: 0.0642
Epoch: 118 train loss: 0.0713 val loss: 0.0295
Epoch: 119 train loss: 0.0586 val loss: 0.0373
Epoch: 120 train loss: 0.0558 val loss: 0.0461
Epoch: 121 train loss: 0.0400 val loss: 0.1046
Epoch: 122 train loss: 0.0975 val loss: 0.1892
Epoch: 123 train loss: 0.0478 val loss: 0.0341
Epoch: 124 train loss: 0.0558 val loss: 0.0443
Epoch: 125 train loss: 0.0803 val loss: 0.0616
Epoch: 126 train loss: 0.0697 val loss: 0.0521
Epoch: 127 train loss: 0.0652 val loss: 0.0849
Epoch: 128 train loss: 0.0485 val loss: 0.0396
Epoch: 129 train loss: 0.0338 val loss: 0.0437
Epoch: 130 train loss: 0.0272 val loss: 0.0825
Epoch: 131 train loss: 0.0309 val loss: 0.0563
Epoch: 132 train loss: 0.0509 val loss: 0.0653
Epoch: 133 train loss: 0.0372 val loss: 0.0437
Epoch: 134 train loss: 0.0427 val loss: 0.0513
Epoch: 135 train loss: 0.0348 val loss: 0.0125
Epoch: 136 train loss: 0.0524 val loss: 0.0687
Epoch: 137 train loss: 0.0621 val loss: 0.0331
Epoch: 138 train loss: 0.0476 val loss: 0.1059
Epoch: 139 train loss: 0.0387 val loss: 0.0843
Epoch: 140 train loss: 0.0484 val loss: 0.0644
Epoch: 141 train loss: 0.0536 val loss: 0.0440
Epoch: 142 train loss: 0.0384 val loss: 0.0970
Epoch: 143 train loss: 0.0289 val loss: 0.0747
Epoch: 144 train loss: 0.0352 val loss: 0.0453
Epoch: 145 train loss: 0.0330 val loss: 0.0740
Epoch: 146 train loss: 0.0290 val loss: 0.0567
Epoch: 147 train loss: 0.0358 val loss: 0.0363
Epoch: 148 train loss: 0.0377 val loss: 0.0630
Epoch: 149 train loss: 0.0492 val loss: 0.0954
Epoch: 150 train loss: 0.0410 val loss: 0.0416
Epoch: 151 train loss: 0.0393 val loss: 0.0579
Epoch: 152 train loss: 0.0235 val loss: 0.0638
Epoch: 153 train loss: 0.0170 val loss: 0.0207
Epoch: 154 train loss: 0.0273 val loss: 0.0274
Epoch: 155 train loss: 0.0255 val loss: 0.0313
Epoch: 156 train loss: 0.0272 val loss: 0.0257
Epoch: 157 train loss: 0.0335 val loss: 0.0489
Epoch: 158 train loss: 0.0502 val loss: 0.0794
Epoch: 159 train loss: 0.0272 val loss: 0.0310
Epoch: 160 train loss: 0.0198 val loss: 0.0103
Epoch: 161 train loss: 0.0139 val loss: 0.0380
Epoch: 162 train loss: 0.0343 val loss: 0.0350
Epoch: 163 train loss: 0.0294 val loss: 0.0626
Epoch: 164 train loss: 0.0151 val loss: 0.0410
Epoch: 165 train loss: 0.0359 val loss: 0.0631
Epoch: 166 train loss: 0.0268 val loss: 0.0443
Epoch: 167 train loss: 0.0136 val loss: 0.0150
Epoch: 168 train loss: 0.0288 val loss: 0.0429
Epoch: 169 train loss: 0.0227 val loss: 0.0588
Epoch: 170 train loss: 0.0314 val loss: 0.0296
Epoch: 171 train loss: 0.0126 val loss: 0.0484
Epoch: 172 train loss: 0.0354 val loss: 0.0369
Epoch: 173 train loss: 0.0187 val loss: 0.0121
Epoch: 174 train loss: 0.0256 val loss: 0.0786
Epoch: 175 train loss: 0.0454 val loss: 0.0296
Epoch: 176 train loss: 0.0126 val loss: 0.0542
Epoch: 177 train loss: 0.0300 val loss: 0.0071
Epoch: 178 train loss: 0.0207 val loss: 0.0765
Epoch: 179 train loss: 0.0294 val loss: 0.0918
Epoch: 180 train loss: 0.0383 val loss: 0.0411
Epoch: 181 train loss: 0.0391 val loss: 0.0595
Epoch: 182 train loss: 0.0253 val loss: 0.0373
Epoch: 183 train loss: 0.0261 val loss: 0.0248
Epoch: 184 train loss: 0.0174 val loss: 0.0105
Epoch: 185 train loss: 0.0096 val loss: 0.0504
Epoch: 186 train loss: 0.0072 val loss: 0.0286
Epoch: 187 train loss: 0.0204 val loss: 0.0246
Epoch: 188 train loss: 0.0165 val loss: 0.0462
Epoch: 189 train loss: 0.0108 val loss: 0.0294
Epoch: 190 train loss: 0.0126 val loss: 0.0579
Epoch: 191 train loss: 0.0097 val loss: 0.0297
Epoch: 192 train loss: 0.0157 val loss: 0.1264
Epoch: 193 train loss: 0.0288 val loss: 0.0535
Epoch: 194 train loss: 0.0387 val loss: 0.0536
Epoch: 195 train loss: 0.0275 val loss: 0.0155
Epoch: 196 train loss: 0.0403 val loss: 0.0423
Epoch: 197 train loss: 0.0451 val loss: 0.0552
Epoch: 198 train loss: 0.0109 val loss: 0.0376
Epoch: 199 train loss: 0.0147 val loss: 0.0302
C:\Users\Admin\Desktop\ComputerVision\BTL\New folder\LSTM_model.py:200: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  output = softmax(output).to('cpu')
top 1 acc: 0.970873786407767
top 5 acc: 1.0
